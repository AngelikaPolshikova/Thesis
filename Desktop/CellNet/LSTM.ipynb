{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b2fb6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.measure import label, regionprops\n",
    "from scipy.ndimage import median_filter\n",
    "from skimage.filters import gaussian\n",
    "from skimage import exposure\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from pathlib import Path\n",
    "desktop_path = Path.home() / \"Desktop\"\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from config import CONFIG\n",
    "import random\n",
    "\n",
    "from tifffile import imread\n",
    "from skimage.transform import resize\n",
    "from scipy.ndimage import median_filter\n",
    "from skimage.filters import gaussian\n",
    "from skimage import exposure\n",
    "from skimage.measure import label, regionprops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d50d2d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class and loader\n",
    "class GraphSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_closest_node(props, manual_xy):\n",
    "    # props: regionprops for the frame\n",
    "    # manual_xy: (x, y) in resized frame coordinates\n",
    "    dists = [np.linalg.norm(np.array(p.centroid[::-1]) - np.array(manual_xy)) for p in props]\n",
    "    return np.argmin(dists)\n",
    "\n",
    "# denoise helper\n",
    "def denoise_gaussian_median(frame, median_size=3, gaussian_sigma=1.0):\n",
    "    medianed = median_filter(frame, size=median_size)\n",
    "    smoothed = gaussian(medianed, sigma=gaussian_sigma)\n",
    "    return smoothed\n",
    "\n",
    "# loading function\n",
    "lost_nodes = {}  # node_id: {'coords': (x, y), 'last_seen': frame_idx}\n",
    "\n",
    "def build_graph_from_frame(\n",
    "    frame,\n",
    "    intensity_thresh=CONFIG[\"intensity_thresh\"],  # Threshold for intensity to consider as signal, custom to Cellnet dataset\n",
    "    max_nodes=CONFIG[\"max_nodes\"],\n",
    "    source_idx=None,\n",
    "    prev_coords=None,\n",
    "    prev_ids=None,\n",
    "    next_id_start=0,\n",
    "    lost_nodes=None,\n",
    "    frame_idx=None,\n",
    "    lost_ttl=CONFIG[\"lost_ttl\"]  # how many frames to keep lost nodes\n",
    "):\n",
    "    # Threshold to create binary mask\n",
    "    binary_mask = frame > intensity_thresh\n",
    "\n",
    "    # Label connected regions (potential signal sources)\n",
    "    labeled = label(binary_mask)\n",
    "\n",
    "    # Extract region properties\n",
    "    props = regionprops(labeled, intensity_image=frame)\n",
    "\n",
    "    node_features = []\n",
    "    coords = []\n",
    "    intensities = []\n",
    "\n",
    "    for i, p in enumerate(props[:max_nodes]):\n",
    "        y, x = p.centroid\n",
    "        intensity = p.mean_intensity\n",
    "\n",
    "        # Extract additional biological/morphological properties\n",
    "        area = p.area\n",
    "        eccentricity = p.eccentricity\n",
    "        solidity = p.solidity\n",
    "        perimeter = p.perimeter\n",
    "\n",
    "        # Build node feature vector\n",
    "        node_feat = [\n",
    "            (x / frame.shape[1]) * 2 - 1,\n",
    "            (y / frame.shape[0]) * 2 - 1,\n",
    "            intensity,\n",
    "            area / (frame.shape[0] * frame.shape[1]),\n",
    "            eccentricity,\n",
    "            solidity,\n",
    "            perimeter / (frame.shape[0] + frame.shape[1])\n",
    "        ]\n",
    "        # Add source indicator\n",
    "        if source_idx is not None and i == source_idx:\n",
    "            node_feat.append(1.0)\n",
    "        else:\n",
    "            node_feat.append(0.0)\n",
    "        node_features.append(node_feat)\n",
    "        coords.append((x, y))\n",
    "        intensities.append(intensity)\n",
    "\n",
    "    # --- Node ID assignment (after collecting all nodes) ---\n",
    "   \n",
    "    # Node tracking threshold\n",
    "    node_tracking_dist_thresh = CONFIG[\"node_tracking_dist_thresh\"]\n",
    "    spatial_edge_thresh = CONFIG[\"spatial_edge_thresh\"]\n",
    "   \n",
    "    ids = []\n",
    "    used_lost_ids = set()\n",
    "    if prev_coords is not None and prev_ids is not None and len(prev_coords) > 0 and len(coords) > 0:\n",
    "        cost_matrix = np.zeros((len(coords), len(prev_coords)))\n",
    "        for i, (x, y) in enumerate(coords):\n",
    "            for j, (px, py) in enumerate(prev_coords):\n",
    "                cost_matrix[i, j] = np.linalg.norm(np.array([x, y]) - np.array([px, py]))\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        assigned_prev = set()\n",
    "        for i in range(len(coords)):\n",
    "            if i in row_ind:\n",
    "                j = col_ind[np.where(row_ind == i)[0][0]]\n",
    "                if cost_matrix[i, j] < node_tracking_dist_thresh:  # 8 is a heuristic for \"maximum allowed movement\" for tracking\n",
    "                    ids.append(prev_ids[j])\n",
    "                    assigned_prev.add(j)\n",
    "                else:\n",
    "                    ids.append(next_id_start)\n",
    "                    next_id_start += 1\n",
    "            else:\n",
    "                ids.append(next_id_start)\n",
    "                next_id_start += 1\n",
    "\n",
    "        # --- LOST NODE MANAGEMENT ---\n",
    "        if lost_nodes is not None and frame_idx is not None:\n",
    "            for j, prev_id in enumerate(prev_ids):\n",
    "                if j not in assigned_prev:\n",
    "                    lost_nodes[prev_id] = {'coords': prev_coords[j], 'last_seen': frame_idx - 1}\n",
    "\n",
    "            # Remove lost nodes that have been lost for too long\n",
    "            to_remove = [nid for nid, info in lost_nodes.items() if frame_idx - info['last_seen'] > lost_ttl]\n",
    "            for nid in to_remove:\n",
    "                del lost_nodes[nid]\n",
    "\n",
    "            # Try to match new nodes to lost nodes\n",
    "            for i, (x, y) in enumerate(coords):\n",
    "                if ids[i] >= next_id_start - max_nodes:  # only for new nodes\n",
    "                    for lost_id, info in lost_nodes.items():\n",
    "                        dist = np.linalg.norm(np.array([x, y]) - np.array(info['coords']))\n",
    "                        if dist < node_tracking_dist_thresh and lost_id not in used_lost_ids:\n",
    "                            ids[i] = lost_id\n",
    "                            used_lost_ids.add(lost_id)\n",
    "                            lost_nodes[lost_id]['last_seen'] = frame_idx\n",
    "                            break\n",
    "\n",
    "    else:\n",
    "        # First frame or no previous nodes\n",
    "        ids = [next_id_start + k for k in range(len(coords))]\n",
    "        next_id_start += len(coords)\n",
    "\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "\n",
    "    # Connect nodes within spatial threshold\n",
    "    for i, (xi, yi) in enumerate(coords):\n",
    "        for j, (xj, yj) in enumerate(coords):\n",
    "            if i != j and (xi - xj)**2 + (yi - yj)**2 < spatial_edge_thresh:  # threshold in pixels (try 16²=256 for 16 pixels, or 20²=400 for 20 pixels)\n",
    "                edge_index.append([i, j])\n",
    "\n",
    "                # --- Edge Features ---\n",
    "                dist = np.sqrt((xi - xj)**2 + (yi - yj)**2) / np.sqrt(frame.shape[0]**2 + frame.shape[1]**2)\n",
    "                delta_intensity = abs(intensities[i] - intensities[j])\n",
    "                angle = np.arctan2(yj - yi, xj - xi) / np.pi\n",
    "\n",
    "                edge_attr.append([dist, delta_intensity, angle])\n",
    "\n",
    "    # Skip empty graphs\n",
    "    if not node_features:\n",
    "        return None, None, None, next_id_start\n",
    "\n",
    "    # Convert to PyTorch Geometric Data object\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "    if edge_attr.ndim == 1:\n",
    "        edge_attr = edge_attr.unsqueeze(1)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    data.node_ids = torch.tensor(ids, dtype=torch.long)\n",
    "    data.y = torch.tensor(intensities, dtype=torch.float).unsqueeze(-1)\n",
    "\n",
    "    return data, coords, ids, next_id_start\n",
    "\n",
    "def load_tif_as_graph_sequences(path, window=5, frame_skip=1, denoise_sigma=1.0, manual_source=None):\n",
    "    \"\"\"\n",
    "    Loads a video as a list of sequences of graphs.\n",
    "    Each sequence is a list of graphs of length 'window'.\n",
    "    Tracks node identities across frames for temporal GNNs.\n",
    "    \"\"\"\n",
    "    stack = imread(path)\n",
    "    stack_denoised = np.array([denoise_gaussian_median(f, gaussian_sigma=denoise_sigma) for f in stack])\n",
    "    stack_resized = np.array([resize(f, CONFIG[\"resize_shape\"]) for f in stack_denoised])\n",
    "    norm_stack = (stack_resized - stack_resized.min()) / (stack_resized.max() - stack_resized.min())\n",
    "    norm_stack = norm_stack * 2 - 1\n",
    "\n",
    "    sequences = []\n",
    "    for i in range(0, len(norm_stack) - window, frame_skip):  # -window instead of -window+1\n",
    "        graph_seq = []\n",
    "        prev_coords, prev_ids, next_id_start = None, None, 0\n",
    "        lost_nodes = {}\n",
    "        frame_indices = []\n",
    "        for j in range(window + 1):  # build window+1 graphs to allow shifting\n",
    "            idx = i + j\n",
    "            temp_props = regionprops(label(norm_stack[idx] > CONFIG[\"intensity_thresh\"]), intensity_image=norm_stack[idx])\n",
    "            if not temp_props:\n",
    "                print(f\"Frame {idx} skipped (no active nodes)\")\n",
    "                break\n",
    "            if manual_source and idx == manual_source['frame']:\n",
    "                source_idx = find_closest_node(temp_props, manual_source['xy'])\n",
    "            else:\n",
    "                source_idx = None\n",
    "            g, coords, ids, next_id_start = build_graph_from_frame(\n",
    "                norm_stack[idx],\n",
    "                source_idx=source_idx,\n",
    "                prev_coords=prev_coords,\n",
    "                prev_ids=prev_ids,\n",
    "                next_id_start=next_id_start,\n",
    "                lost_nodes=lost_nodes,\n",
    "                frame_idx=idx,\n",
    "                lost_ttl=CONFIG[\"lost_ttl\"]\n",
    "            )\n",
    "            if g is None:\n",
    "                break\n",
    "            g.frame_idx = idx\n",
    "            g.source_file = path\n",
    "            prev_coords, prev_ids = coords, ids\n",
    "            graph_seq.append(g)\n",
    "            frame_indices.append(idx)\n",
    "        if len(graph_seq) == window + 1:\n",
    "            # Shift targets: for each t, set g.y = next_g.y for matching node_ids\n",
    "            for t in range(window):\n",
    "                curr_g = graph_seq[t]\n",
    "                next_g = graph_seq[t + 1]\n",
    "                # Map node ids to indices\n",
    "                curr_ids = curr_g.node_ids.cpu().numpy()\n",
    "                next_ids = next_g.node_ids.cpu().numpy()\n",
    "                next_id_to_idx = {nid: i for i, nid in enumerate(next_ids)}\n",
    "                # For each node in curr_g, if it exists in next_g, set target to next_g.y, else set to nan\n",
    "                new_y = []\n",
    "                for nid in curr_ids:\n",
    "                    if nid in next_id_to_idx:\n",
    "                        new_y.append(next_g.y[next_id_to_idx[nid]])\n",
    "                    else:\n",
    "                        new_y.append(torch.tensor([float('nan')], dtype=torch.float))\n",
    "                curr_g.y = torch.stack(new_y)\n",
    "            # Only keep the first window graphs (targets are next-frame)\n",
    "            sequences.append(graph_seq[:window])\n",
    "    return sequences\n",
    "file_paths = [\n",
    "    'C:/Users/Platypus/Documents/CellNet/Real_Time_CS_Experiment-1093.tif',\n",
    "    'C:/Users/Platypus/Documents/CellNet/Flow prior to chemical stimulation_Figure6C.tif', #✔ Clicked on Frame 77: (1.16, 2.96); ✔ Clicked on Frame 77: (15.57, 10.86)\n",
    "    'C:/Users/Platypus/Documents/CellNet/Figure8.tif',\n",
    "    'C:/Users/Platypus/Documents/CellNet/5uM_per_litre_Figure6_ChemicalStimulation.tif'\n",
    "]\n",
    "test_video_path = 'C:/Users/Platypus/Documents/CellNet/Cell Knocked_Figure7.tif'\n",
    "\n",
    "\n",
    "\n",
    "#sources where signal starts\n",
    "manual_sources = {\n",
    "    'Real_Time_CS_Experiment-1093.tif': {'frame': 5, 'xy': (3.79, 3.66)}, #altenatively \"Clicked on Frame 5: (24.44, 10.58)\"\n",
    "    'Flow prior to chemical stimulation_Figure6C.tif': { 'frame': 77, 'xy': (1.16, 2.96)}, #alt ✔ Clicked on Frame 77: (15.57, 10.86)\n",
    "    'Figure8.tif': {'frame': 68, 'xy': (43.83, 0.89)}, \n",
    "    '5uM_per_litre_Figure6_ChemicalStimulation.tif':{'frame': 37, 'xy': (10.03, 5.46)},# alt Frame 76: (2.82, 3.38);  \n",
    "    'Cell Knocked_Figure7.tif': {'frame': 0, 'xy': (54.50, 9.34)} #Clicked on Frame 0: (54.50, 9.34);alt ✔ Clicked on Frame 0: (53.66, 3.52)\n",
    "    }\n",
    "test_sequences = load_tif_as_graph_sequences(test_video_path, window=5, manual_source=manual_sources.get(os.path.basename(test_video_path)))\n",
    "test_dataset = GraphSequenceDataset(test_sequences)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, collate_fn=lambda x: x[0])\n",
    "\n",
    "\n",
    "class GraphSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        \"\"\"\n",
    "        sequences: list of list of PyG Data objects (each inner list is a sequence of graphs)\n",
    "        \"\"\"\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Returns a list of Data objects (the sequence)\n",
    "        return self.sequences[idx]\n",
    "\n",
    "def graph_sequence_collate(batch):\n",
    "    # batch: list of sequences (each is a list of Data objects)\n",
    "    # Output: list of length window, each element is a list of Data objects for that time step\n",
    "    # For batch_size=1, this just returns [sequence]\n",
    "    # For batch_size>1, this stacks sequences by time step\n",
    "    batch_size = len(batch)\n",
    "    seq_len = len(batch[0])\n",
    "    collated = []\n",
    "    for t in range(seq_len):\n",
    "        collated.append([batch[b][t] for b in range(batch_size)])\n",
    "    return collated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75b22b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_baseline(graph_seq):\n",
    "    # Predict each node’s intensity as it was in the previous frame\n",
    "    pred = []\n",
    "    target = []\n",
    "    for t in range(1, len(graph_seq)):\n",
    "        prev = graph_seq[t-1]\n",
    "        curr = graph_seq[t]\n",
    "\n",
    "        # Match node_ids between frames\n",
    "        id_to_idx_prev = {nid.item(): i for i, nid in enumerate(prev.node_ids)}\n",
    "        id_to_idx_curr = {nid.item(): i for i, nid in enumerate(curr.node_ids)}\n",
    "\n",
    "        common_ids = set(id_to_idx_prev.keys()).intersection(id_to_idx_curr.keys())\n",
    "        if not common_ids:\n",
    "            continue\n",
    "\n",
    "        pred_t = []\n",
    "        target_t = []\n",
    "        for nid in common_ids:\n",
    "            pred_t.append(prev.y[id_to_idx_prev[nid]].item())\n",
    "            target_t.append(curr.y[id_to_idx_curr[nid]].item())\n",
    "\n",
    "        pred.append(pred_t)\n",
    "        target.append(target_t)\n",
    "\n",
    "    pred_flat = np.concatenate(pred)\n",
    "    target_flat = np.concatenate(target)\n",
    "    loss = np.mean(np.abs(pred_flat - target_flat))\n",
    "    corr, _ = pearsonr(pred_flat, target_flat)\n",
    "    return loss, corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa016fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMIntensityModel(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=64, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, 1]\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.out  # [batch, seq_len, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a3abf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lstm_data(graph_seq):\n",
    "    # Align nodes by ID across frames\n",
    "    node_id_lists = [g.node_ids.cpu().numpy() for g in graph_seq]\n",
    "    common_ids = set(node_id_lists[0])\n",
    "    for ids in node_id_lists[1:]:\n",
    "        common_ids &= set(ids)\n",
    "    common_ids = sorted(list(common_ids))\n",
    "    if not common_ids:\n",
    "        return None, None\n",
    "\n",
    "    seq_data = []\n",
    "    for g in graph_seq:\n",
    "        id_to_idx = {nid.item(): i for i, nid in enumerate(g.node_ids)}\n",
    "        intensities = [g.y[id_to_idx[nid]].item() for nid in common_ids]\n",
    "        seq_data.append(intensities)\n",
    "\n",
    "    seq_data = torch.tensor(seq_data).unsqueeze(-1).float()  # [seq_len, num_nodes, 1]\n",
    "    seq_data = seq_data.permute(1, 0, 2)  # [num_nodes, seq_len, 1]\n",
    "    return seq_data[:, :-1, :], seq_data[:, 1:, :]  # X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71deb256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model(graph_seqs, epochs=20):\n",
    "    model = LSTMIntensityModel()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.L1Loss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        for graph_seq in graph_seqs:\n",
    "            model.train()\n",
    "            out = prepare_lstm_data(graph_seq)\n",
    "            if out is None:\n",
    "                continue\n",
    "            x, y = out\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        print(f\"[LSTM] Epoch {epoch+1}: MAE = {np.mean(losses):.4f}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee332c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lstm_model(model, graph_seqs):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for graph_seq in graph_seqs:\n",
    "            out = prepare_lstm_data(graph_seq)\n",
    "            if out is None:\n",
    "                continue\n",
    "            x, y = out\n",
    "            pred = model(x)\n",
    "            preds.append(pred.flatten().numpy())\n",
    "            trues.append(y.flatten().numpy())\n",
    "    pred_flat = np.concatenate(preds)\n",
    "    true_flat = np.concatenate(trues)\n",
    "    loss = np.mean(np.abs(pred_flat - true_flat))\n",
    "    corr, _ = pearsonr(pred_flat, true_flat)\n",
    "    return loss, corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c2e90bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loaded sequences: 4105\n",
      "Naive Baseline - MAE: nan | Pearson: nan\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Linear' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNaive Baseline - MAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_naive\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Pearson: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorr_naive\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# LSTM baseline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m lstm_model = \u001b[43mtrain_lstm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# train on small subset\u001b[39;00m\n\u001b[32m     32\u001b[39m loss_lstm, corr_lstm = evaluate_lstm_model(lstm_model, test_loader.dataset[:\u001b[32m20\u001b[39m])\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLSTM Baseline - MAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_lstm\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Pearson: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorr_lstm\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtrain_lstm_model\u001b[39m\u001b[34m(graph_seqs, epochs)\u001b[39m\n\u001b[32m     13\u001b[39m x, y = out\n\u001b[32m     14\u001b[39m pred = model(x)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m loss = \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m optimizer.zero_grad()\n\u001b[32m     17\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Platypus\\Documents\\CellNet\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Platypus\\Documents\\CellNet\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Platypus\\Documents\\CellNet\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:128\u001b[39m, in \u001b[36mL1Loss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43ml1_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Platypus\\Documents\\CellNet\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3743\u001b[39m, in \u001b[36ml1_loss\u001b[39m\u001b[34m(input, target, size_average, reduce, reduction)\u001b[39m\n\u001b[32m   3733\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target):\n\u001b[32m   3734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m   3735\u001b[39m         l1_loss,\n\u001b[32m   3736\u001b[39m         (\u001b[38;5;28minput\u001b[39m, target),\n\u001b[32m   (...)\u001b[39m\u001b[32m   3741\u001b[39m         reduction=reduction,\n\u001b[32m   3742\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3743\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target.size() == \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m()):\n\u001b[32m   3744\u001b[39m     warnings.warn(\n\u001b[32m   3745\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m). \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3746\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3747\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure they have the same size.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3748\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   3749\u001b[39m     )\n\u001b[32m   3750\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Platypus\\Documents\\CellNet\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1929\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1930\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1931\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1932\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1933\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'Linear' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "# Naive baseline\n",
    "\n",
    "\n",
    "\n",
    "all_sequences = []\n",
    "for path in file_paths:\n",
    "    if os.path.exists(path):\n",
    "        video_name = os.path.basename(path)\n",
    "        manual_source = manual_sources.get(video_name)\n",
    "        seqs = load_tif_as_graph_sequences(path, window=5, manual_source=manual_source)\n",
    "        all_sequences.extend(seqs)\n",
    "print(f\"Total loaded sequences: {len(all_sequences)}\")\n",
    "\n",
    "train_split = int(0.7 * len(all_sequences))\n",
    "val_split = int(0.15 * len(all_sequences))\n",
    "\n",
    "train_sequences = all_sequences[:train_split]\n",
    "val_sequences = all_sequences[train_split:train_split + val_split]\n",
    "test_sequences = all_sequences[train_split + val_split:]\n",
    "\n",
    "train_dataset = GraphSequenceDataset(train_sequences)\n",
    "test_dataset = GraphSequenceDataset(test_sequences)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=lambda x: x[0])\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, collate_fn=lambda x: x[0])\n",
    "\n",
    "loss_naive, corr_naive = naive_baseline(test_loader.dataset[0])\n",
    "print(f\"Naive Baseline - MAE: {loss_naive:.4f} | Pearson: {corr_naive:.4f}\")\n",
    "\n",
    "# LSTM baseline\n",
    "lstm_model = train_lstm_model(train_loader.dataset[:20])  # train on small subset\n",
    "loss_lstm, corr_lstm = evaluate_lstm_model(lstm_model, test_loader.dataset[:20])\n",
    "print(f\"LSTM Baseline - MAE: {loss_lstm:.4f} | Pearson: {corr_lstm:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
